{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38713f73-7524-4297-bf75-7ed08fe4e531",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "A description of the deep residual learning framework and its working principle in the ResNet paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c56eb-c9b6-4221-8bd4-bcceeb0d2e1e",
   "metadata": {},
   "source": [
    "### Deep Residual Framework\n",
    "\n",
    "```Work in Progress```\n",
    "\n",
    "![image](../images/Residual_Module.png)\n",
    "```put ref on image```\n",
    "<!--![Picture](../images/Residual_Module.png){width=\"800\" height=\"600\" style=\"display: block; margin: 0 auto\"}-->\n",
    "\n",
    " \n",
    "<!--<div>\n",
    "<img src=\"../images/Residual_Module.png\" width=\"300\" align=\"right\"/>\n",
    "</div>-->\n",
    "\n",
    "Let us define $\\mathcal{H}(x)$ as the underlying mapping that a stack of layers should be able to fit, and let $x$ represent the input of the first layer (assuming both have the same dimensions).\n",
    "\n",
    "If one hypothesizes that any complicated function can be asymptotically approximated by multiple nonlinear layers, the same should hold true for the residual functions, i.e., $\\mathcal{H}(x) - x$.\n",
    "\n",
    "The authors of the paper (add reference here) explicitly let these layers approximate a residual function:\n",
    "\n",
    "$$F(x):=H(x)−x$$\n",
    "\n",
    "while the original mapping becomes:\n",
    "\n",
    "$$H(x)=F(x)+x.$$\n",
    "\n",
    "\n",
    "Although both forms should asymptotically approximate the desired functions (as hypothesized), the ease of learning might differ.\n",
    "\n",
    "This reformulation is motivated by the counterintuitive phenomenon of the degradation problem, where deeper models experience a degradation of the training error.\n",
    "\n",
    "Intuitively, a deeper model constructed from a shallower one by adding layers (which act as identity maps) should not experience a greater training error.\n",
    "\n",
    "The degradation problem, however, suggests that solvers might have difficulties in approximating identity mappings with multiple nonlinear layers. This issue can be mitigated by reformulating the problem using residuals, where the weights are driven toward zero.\n",
    "\n",
    "If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find perturbations relative to an identity mapping than to learn the function from scratch.\n",
    "\n",
    "\n",
    "### The ResNet structure\n",
    "\n",
    "The formulation $F(x)+x$ can be implemented in feedforward neural networks using \"shortcut connections\" (as illustrated in the previous figure).\n",
    "These connections skip one or more layers, performing a simple identity mapping, and their outputs are added to the outputs of the stacked layers.\n",
    "\n",
    "An important detail is that identity shortcut connections do not introduce additional parameters or computational complexity.\n",
    "\n",
    "The model primarily consists of convolutional layers with 3×3 filters and follows two simple design rules:\n",
    "1. For the same output feature map size, all layers use the same number of filters.\n",
    "2. When the feature map size is halved, the number of filters is doubled to preserve the time complexity per layer.\n",
    "\n",
    "Downsampling is performed directly by convolutional layers with a stride of 2. The network concludes with a global average pooling layer and a 1000-way fully connected layer with a softmax activation function (used to solve the ILSVRC 2015 classification task).\n",
    "\n",
    "To this structure, the authors introduce shortcut connections, transforming the network into its residual counterpart. When the dimensions between input and output differ, two options are proposed:\n",
    "- The shortcut performs identity mapping, with extra zero entries padded to match the increased dimensions:\n",
    "$$y = \\mathcal{F}(x, {W_{i} }) + x$$\n",
    "- A projection shortcut is used to match dimensions via 1×1 convolutions:\n",
    "$$y = \\mathcal{F}(x, {W_{i} }) + W_{s} x$$\n",
    "\n",
    "For both options, when the shortcuts span feature maps of different sizes, they are performed with a stride of 2.\n",
    "\n",
    "#### Full Training setup:\n",
    "- The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation.\n",
    "- A 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted. \n",
    "- The standard color augmentation is used. \n",
    "- **We adopt batch normalization (BN) right after each convolution and before activation.**\n",
    "- We initialize the weights and train from scratch. \n",
    "- We use SGD with a **mini-batch size of 256**. \n",
    "- The learning rate starts from 0.1 and is divided by 10 when the error plateaus.\n",
    "- The models are trained for up to **60 × 104 iterations**. \n",
    "- We use a weight decay of 0.0001 and a momentum of 0.9. \n",
    "- We do not use dropout.\n",
    "- In testing, for comparison studies we adopt the standard 10-crop testing. \n",
    "- For best results, we adopt the fully-convolutional form, and average the scores at multiple scales (images are resized such that the shorter side is in {224, 256, 384, 480, 640})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab026cdd-6234-4dfa-ae67-bec43cf59df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
